{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb928c5",
   "metadata": {},
   "source": [
    "# Tutorial 7-1. Natural Language Processing (NLP)\n",
    "\n",
    "**GOAL**: Let's taste how to process natural language text using Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a2644",
   "metadata": {},
   "source": [
    "### 0. Installation\n",
    "\n",
    "Before starting this tutorial, please be prepared by installing the two packages.\n",
    "- English: `nltk` package\n",
    "- Korean: `konlpy` package (+Java Development Kit)\n",
    "\n",
    "You can install them by running the following lines in Anaconda Prompt:\n",
    "```\n",
    ">> conda install nltk\n",
    ">> pip install konlpy\n",
    "```\n",
    "\n",
    "For details, see slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f620b",
   "metadata": {},
   "source": [
    "### 1. English NLP using NLTK package\n",
    "\n",
    "[NLTK](https://www.nltk.org/) is a pioneering NLP package built in Python.\n",
    "\n",
    "First, import the `nltk` package and download a tokenizer and a pos tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d23b6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\01wkd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\01wkd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt') # tokenizer\n",
    "nltk.download('averaged_perceptron_tagger') # pos tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c16015",
   "metadata": {},
   "source": [
    "Try your own sentences. You can tokenize, POS tag, or extract nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d50d4e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), (\"'d\", 'MD'), ('like', 'VB'), ('to', 'TO'), ('drink', 'VB'), ('some', 'DT'), ('water', 'NN'), ('.', '.'), ('But', 'CC'), ('I', 'PRP'), ('have', 'VBP'), ('only', 'RB'), ('beer', 'NN'), ('.', '.')]\n",
      "['water', 'beer']\n",
      "['water', 'beer']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I'd like to drink some water. But I have only beer.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)\n",
    "\n",
    "nn_list = []\n",
    "for word, tag in tagged:\n",
    "    if tag[:2] == 'NN' :\n",
    "        nn_list.append(word)\n",
    "\n",
    "nouns = [word for (word, tag) in tagged if tag[:2] == 'NN']\n",
    "print(nn_list)\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "164879be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['by', 'the', 'end', 'of', 'this', 'course', 'i', 'will', 'be', 'a', 'great', 'data', 'scientist', '!', '100', '%', 'sure', '!']\n",
      "[('by', 'IN'), ('the', 'DT'), ('end', 'NN'), ('of', 'IN'), ('this', 'DT'), ('course', 'NN'), ('i', 'NN'), ('will', 'MD'), ('be', 'VB'), ('a', 'DT'), ('great', 'JJ'), ('data', 'NNS'), ('scientist', 'NN'), ('!', '.'), ('100', 'CD'), ('%', 'NN'), ('sure', 'JJ'), ('!', '.')]\n",
      "['end', 'course', 'i', 'data', 'scientist', '%']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"By the end of this course I will be a great data scientist! 100% sure!\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence.lower()) # lowercase\n",
    "print(tokens)\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)\n",
    "\n",
    "nouns = [word for word, tag in tagged if tag[:2] == 'NN']\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f32beaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f91b672",
   "metadata": {},
   "source": [
    "### 2. Korean NLP using KoNLPy pacakge\n",
    "\n",
    "[KoNLPy](https://konlpy.org/) is a Python package for NLP of Korean language.\n",
    "\n",
    "It contains famous Korean POS taggers such as Hannanum (`Hannanum`), Kkma(`Kkma`), and Open Korean Text (`Okt`, aka Twitter).\n",
    "\n",
    "Here, we will use Twitter tagger. Let's load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7c50153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7353c117",
   "metadata": {},
   "source": [
    "Try your own sentence. You can tokenize, POS tag, or extract nouns and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "920d975d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['공부', '를', '하면', '할수록', '모르는게', '많다는', '것', '을', '알', '게', '됩니다', '.', '배운건', '많았는데', '...', '다', '까먹어', '버렸네요', '?', 'ㅋㅋ', '그래도', '계속', '공부', '합니다', '.', '재밌으니까', '!']\n",
      "[('공부', 'Noun'), ('를', 'Josa'), ('하면', 'Verb'), ('할수록', 'Verb'), ('모르는게', 'Verb'), ('많다는', 'Adjective'), ('것', 'Noun'), ('을', 'Josa'), ('알', 'Noun'), ('게', 'Josa'), ('됩니다', 'Verb'), ('.', 'Punctuation'), ('배운건', 'Verb'), ('많았는데', 'Adjective'), ('...', 'Punctuation'), ('다', 'Adverb'), ('까먹어', 'Verb'), ('버렸네요', 'Verb'), ('?', 'Punctuation'), ('ㅋㅋ', 'KoreanParticle'), ('그래도', 'Adverb'), ('계속', 'Noun'), ('공부', 'Noun'), ('합니다', 'Verb'), ('.', 'Punctuation'), ('재밌으니까', 'Adjective'), ('!', 'Punctuation')]\n",
      "['공부', '것', '알', '계속', '공부']\n",
      "['공부', '많다는 것', '계속', '계속 공부']\n"
     ]
    }
   ],
   "source": [
    "# sentence = '데이터 분석 수업이 넘나 재밌어서 현기증이 나요...ㅋ_ㅠ'\n",
    "sentence = '공부를 하면할수록 모르는게 많다는 것을 알게 됩니다. 배운건 많았는데... 다 까먹어버렸네요? ㅋㅋ 그래도 계속 공부합니다. 재밌으니까!'\n",
    "\n",
    "tokens = okt.morphs(sentence)\n",
    "print(tokens)\n",
    "\n",
    "tagged = okt.pos(sentence)\n",
    "print(tagged)\n",
    "\n",
    "nouns = okt.nouns(sentence)\n",
    "print(nouns)\n",
    "\n",
    "phrases = okt.phrases(sentence)\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9e89f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
